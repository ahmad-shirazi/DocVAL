# DocVAL: Visual Answer Localization for Document VQA

Training compact Vision-Language Models to perform Document Visual Question Answering with spatial grounding using asymmetric text detection and rule-based validation feedback.

## Overview

DocVAL distills knowledge from large teacher VLMs (Gemini 2.5 Pro) into compact student models (Gemma 3-12B) that can:
- Answer questions about documents
- Localize answers with bounding boxes  
- Explain reasoning with Chain-of-Thought
- Run efficiently without OCR at inference

![DocVAL Architecture](imgs/docval.png)

*DocVAL's three-phase pipeline: Teacher data generation with VAL filtering (Phase A), two-stage student training (Phase B), and pure VLM inference deployment (Phase C).*

## Quick Start

### Installation

```bash
git clone https://github.com/your-username/docval.git
cd docval
python3 -m venv envval
source envval/bin/activate
pip install -r requirements.txt
pip install -e .
```

### Setup

Create `.env` file with your API keys:

```bash
HF_TOKEN=your_huggingface_token
GEMINI_API_KEY=your_gemini_api_key
```

### Run Pipeline

```bash
# Phase A: Generate CoT data + VAL filtering
python -m docval.scripts.run_phase_a --datasets CORD DocVQA FUNSD SROIE VisualMRC
python -m docval.scripts.merge_cot_datasets
python -m docval.scripts.apply_val_filter

# Phase B: Train student model
python -m docval.training.train_b1_mac_full

# Phase C: Evaluate
python -m docval.scripts.run_phase_c_mac
```

## Dataset & Pretrained Data

All datasets and generated CoT data are available for the community:

**ðŸ”— [Download from Google Drive](https://drive.google.com/drive/folders/1PxWmr-5XGsGVKXWFkUKc3j8ptACLhjQn?usp=sharing)**

### Available Files

**Datasets** (raw):
- CORD - Receipts
- DocVQA - Documents  
- FUNSD - Forms
- SROIE - Receipts
- VisualMRC - Web screenshots

**Generated CoT Data**:
- `cot_cord.json` (300 KB)
- `cot_docvqa.json` (71.1 MB)
- `cot_funsd.json` (628 KB)
- `cot_sroie.json` (8.2 MB)
- `cot_visualmrc.json` (21.3 MB)
- `cot_merged_all.json` (100.3 MB) - All datasets merged

You can use these pre-generated files to skip Phase A and directly start training!

## Project Structure

```
DocVAL/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ setup.py                  # Package setup
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ PHASE_A_GUIDE.md
â”‚   â”œâ”€â”€ PHASE_B1_TRAINING_GUIDE.md
â”‚   â”œâ”€â”€ PHASE_B2_GUIDE.md
â”‚   â”œâ”€â”€ PHASE_C_GUIDE.md
â”‚   â””â”€â”€ MAC_TRAINING_GUIDE.md
â”‚
â””â”€â”€ docval/                   # Main package
    â”œâ”€â”€ config/              # Configuration
    â”œâ”€â”€ data/                # Data processing
    â”œâ”€â”€ models/              # Teacher/student VLMs
    â”œâ”€â”€ training/            # Training scripts
    â”œâ”€â”€ validation/          # VAL filter system
    â”œâ”€â”€ inference/           # Evaluation
    â”œâ”€â”€ scripts/             # Utility scripts
    â””â”€â”€ utils/               # Utilities
```

## Three-Phase Pipeline

### Phase A: Teacher Data Generation
- Generate CoT examples using Gemini 2.5 Pro + DB-ResNet
- Apply VAL filter (5-module quality assessment)
- Output: 2,363 high-quality examples

### Phase B: Student Training
- **B1**: Fine-tune Gemma 3-12B on CoT data
- **B2**: Iterative refinement with VAL feedback (optional)

### Phase C: Evaluation
- Test student model on held-out set
- Pure VLM inference (no OCR)
- Compute ANLS (answer) & IoU (bbox) metrics

## Key Features

- âœ… **Asymmetric Detection**: OCR only during training
- âœ… **VAL Filter**: Rule-based quality assessment
- âœ… **Iterative Refinement**: Dynamic correction dataset
- âœ… **Mac Support**: Optimized for Apple Silicon
- âœ… **Full Fine-Tuning**: Maximum spatial reasoning

![VAL Filter System](imgs/val.png)

*VAL (Visual Answer Localization) Filter: 5-module quality assessment system that validates OCR grounding, answer quality (ANLS), bbox quality (IoU), reasoning quality, and computes an overall quality score.*

## Supported Models

**Teacher**: Gemini 2.5 Pro (default), GPT-5, Claude 4.5, Gemini 2.5 Flash, GPT-4o  
**Student**: Gemma 3-12B (default), Gemma 3-4B, Qwen3-VL-8B, InternVL3.5, Llama-3.2-11B  
**Detector**: DB-ResNet50 (default), CRAFT, PSENet, PaddleOCR, EasyOCR

## Hardware Requirements

**Minimum** (LoRA): RTX 3090 (24GB) or Mac M1 Pro, 32GB RAM  
**Recommended** (Full): A100 (80GB) or Mac M4 Max (64GB), 64GB RAM

## Documentation

- **[Phase A Guide](docs/PHASE_A_GUIDE.md)** - Teacher data generation
- **[Phase B1 Guide](docs/PHASE_B1_TRAINING_GUIDE.md)** - Student training
- **[Phase B2 Guide](docs/PHASE_B2_GUIDE.md)** - VAL feedback training
- **[Phase C Guide](docs/PHASE_C_GUIDE.md)** - Evaluation
- **[Mac Guide](docs/MAC_TRAINING_GUIDE.md)** - Mac-specific instructions

## Command Reference

```bash
# Phase A
python -m docval.scripts.run_phase_a --datasets CORD DocVQA FUNSD SROIE VisualMRC
python -m docval.scripts.merge_cot_datasets
python -m docval.scripts.apply_val_filter

# Phase B
python -m docval.training.train_b1_mac_full            # Full fine-tuning (Mac)
python -m docval.training.train_phase_b1 --use-lora    # LoRA (faster)
python -m docval.training.train_b2_mac                 # VAL feedback (Mac)

# Phase C
python -m docval.scripts.run_phase_c_mac

# Utilities
python -m docval.scripts.monitor_phase_a     # Monitor progress
python -m docval.scripts.show_metrics        # View metrics
python -m docval.scripts.check_progress      # Check status
```

## Results

![Performance Results](imgs/results.png)

*DocVAL achieves competitive performance on document VQA benchmarks with efficient inference. The student model (Gemma 3-12B) successfully learns spatial reasoning from teacher-generated CoT data with VAL filtering.*
